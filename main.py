# main.py
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# 1Ô∏è‚É£ Load speech text
loader = TextLoader("speech.txt", encoding="utf-8")
documents = loader.load()

# 2Ô∏è‚É£ Split text into chunks
text_splitter = CharacterTextSplitter(
    chunk_size=500,  # adjust chunk size if needed
    chunk_overlap=50
)
docs = text_splitter.split_documents(documents)

# 3Ô∏è‚É£ Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4Ô∏è‚É£ Create or load local Chroma vector store
vectordb = Chroma.from_documents(docs, embeddings, persist_directory="chroma_db")
vectordb.persist()

# 5Ô∏è‚É£ Setup Ollama LLM
llm = Ollama(model="mistral")

# 6Ô∏è‚É£ Create Retrieval QA chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",   # simple approach
    retriever=vectordb.as_retriever()
)

# 7Ô∏è‚É£ Interactive Q&A loop
print("üí° Welcome to Dr. B.R. Ambedkar Speech Q&A. Type 'exit' to quit.")

while True:
    query = input("\nEnter your question: ")
    if query.lower() in ["exit", "quit"]:
        break
    answer = qa.run(query)
    print("\nüìù Answer:", answer)
